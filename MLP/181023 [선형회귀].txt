[python-MLP]
[딥러닝]
# 미분식을 합쳐서 x에 상수 1 칼럼을 추가

[Elastic net regularization]
# L1 + L2 정규화 : L1은 라소, L2는 리지
 -> 러닝레이트에 더불어, 세타의 합

[tf]
# reduce_sum
 -> 총합 --> 차원감소
# random.normal(size=5) -> size는 5개
# 리스트는 [:,] index가 안된다.
# optimizer
 -> tf.train.GradientDescentOptimizer(learning_rate)
 -> train = opt.minimize(cost)
# plt.legend -> 범례

#[softmax]
var= tf.one_hot(y, classes) --> 차원이 하나 증가한다
var = tf.reshape(var, [-1, classes])
tf.nn.l2_loss(w) -> l2 정규화
hypothesis = tf.nn.nsoftmax(logit)
logit = tf.matmul(x, W) + b